{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <font color = 'orangered' size=6>Libraries import</font>   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import re\n",
        "import scipy\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from statannot import add_stat_annotation\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from math import pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SP-hpQV5qzG"
      },
      "source": [
        "# <font color = 'orangered' size=6>Image preprocessing </font>   \n",
        "\n",
        "# <font color = '#32CD32'>0. Nifti dimentions converter </font>\n",
        "\n",
        "><font size='5'> 1. Uploads file from '**folder_path**' </font>\n",
        ">\n",
        "><font size='5'> 2. Please type your voxel size (um) after cell initialization one by one in (X, Y, Z) order  </font>\n",
        ">\n",
        "><font size='5'> 3. Returns converted files to '**out_path**' folder </font>\n",
        "\n",
        "\n",
        "The '**matlab_script_converter_nifti**', '**folder_path**' and '**out_path**' need to be defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej_lYXCO5qzH",
        "outputId": "6aa344e0-13f8-40f0-8a2d-f9c15d7a2b5c"
      },
      "outputs": [],
      "source": [
        "matlab_script_path = r'C:\\Folder_with_images\\convertNIfTI.m'  # Define path to the MATLAB script\n",
        "\n",
        "folder_path = r'C:\\Folder_with_images'  # Define the path to the folder containing the images\n",
        "\n",
        "out_path = rf'{folder_path}\\Scaled'  # Define the path to an output folder\n",
        "\n",
        "messages = ['Input X (um): ', 'Input Y (um): ', 'Input Z (um): ']   # Voxel dimentions settings\n",
        "dim1, dim2, dim3 = [float(input(msg)) for msg in messages]\n",
        "matlab_script_dir = os.path.dirname(matlab_script_path)\n",
        "matlab_command = f'matlab -nodesktop -nosplash -r \"cd(\\'{matlab_script_dir}\\'); convertNIfTI(\\'{folder_path}\\', \\'{out_path}\\', {dim1}, {dim2}, {dim3}); exit;\"'\n",
        "subprocess.run(matlab_command, shell=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMkBuVqX5qzI"
      },
      "source": [
        "\n",
        "# <font color = '#32CD32'>1.  Paths and tags</font>\n",
        "\n",
        "\n",
        "><font size='5'>1. Define the path to the folder with images after scaling\n",
        ">  \n",
        "><font size='5'>2. Select the files of interest by specifying the 'tag'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63SgmCR-5qzK",
        "outputId": "47364963-814a-4ef4-a7ae-abc4b3e59dcc",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "folder_path = input(\"Enter the folder path: \")\n",
        "tag = input(\"Enter the tag for file sorting: \")\n",
        "\n",
        "print(f'Introduced path:                      {folder_path}')\n",
        "print(f'Introduced tag for file selection:    {tag}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNBiad6_5qzL"
      },
      "source": [
        "# Sort files and copy files into a new folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRuxss985qzL",
        "outputId": "2633a9a8-f658-4bc9-aea8-13fe73f694a3"
      },
      "outputs": [],
      "source": [
        "def copy_and_rename_files(folder_path, tag):\n",
        "\n",
        "    \"\"\"\n",
        "        Function creates a new folder with the name specified by the tag parameter within the given folder_path.\n",
        "        A list of files is retrieved and sorted alphabetically in the folder_path.\n",
        "        The function iterates over each file in the sorted list and extracts the file name without the extension.\n",
        "        A new file name is constructed using the extracted file name and the provided tag.\n",
        "        If the tag is detected in the file name, the function proceeds to the subsequent steps.\n",
        "          Otherwise, a message is printed, indicating that the file name does not contain the matching tag.\n",
        "        A new folder and a file path are created by combining the tag and the previous file path.\n",
        "        The file is copied to the new file path using shutil.copy2.\n",
        "        If the file is being used by another process, the copying operation is repeated up to max_retries times with a delay of 0.1 seconds between repeats.\n",
        "        If the copying and renaming is successful, a message is printed, indicating the original file name and the new file name in the new folder.\n",
        "    \"\"\"\n",
        "\n",
        "    tagname_folder = os.path.join(folder_path, f'{tag}')\n",
        "    if not os.path.exists(tagname_folder):\n",
        "        os.makedirs(tagname_folder)\n",
        "    files = os.listdir(folder_path)\n",
        "    files.sort()\n",
        "    for i, file_name in enumerate(files):\n",
        "\n",
        "        if tag in file_name:\n",
        "            file_name_for_strings = file_name.split(\".\")[0]\n",
        "            new_file_name = f\"{file_name_for_strings}\"\n",
        "            file_extension = os.path.splitext(file_name)[1]\n",
        "            new_file_path = os.path.join(tagname_folder, new_file_name + file_extension)\n",
        "            max_retries = 3\n",
        "            current_retry = 0\n",
        "            while current_retry < max_retries:\n",
        "                    try:\n",
        "                        shutil.copy2(os.path.join(folder_path, file_name), new_file_path)\n",
        "                        print(f\"Copied '{file_name}' as '{new_file_name}{file_extension}' to new folder.\")\n",
        "                        break\n",
        "                    except PermissionError:\n",
        "                        current_retry += 1\n",
        "        else:\n",
        "            print(f'{file_name}  The filename does not match the pattern')\n",
        "\n",
        "copy_and_rename_files(folder_path, tag)             # Call the function to copy and rename the files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ZPOprU5qzM"
      },
      "source": [
        "# <font color = '#32CD32'>2. Image preparation</font>\n",
        "\n",
        "# Provide the path and tag of files to the Avizo 'Main Python Console'\n",
        "\n",
        "><font size='5'>Run the cell and paste the following **text output** into Avizo 'Main Python Console'</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKflLdp45qzM",
        "outputId": "ca337a59-98ba-4f6a-f4e8-3ccd1f49a320"
      },
      "outputs": [],
      "source": [
        "new_path = f'{folder_path}\\{tag}'\n",
        "\n",
        "print(f\"new_path = r'{new_path}'\")\n",
        "print(f\"tag = '{tag}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgsJzpRq5qzN"
      },
      "source": [
        "# <font>ROI selection (this step is optional, if not needed, you can skip it and move to Image Processing)</font>\n",
        "\n",
        "><font size='5'>Paste the following **code** into Avizo console</font>\n",
        ">\n",
        "><font size='5'>Now you will work in the Avizo project window: </font>  \n",
        ">\n",
        "><font size='5'>Your images are uploaded</font>  \n",
        ">\n",
        "><font size='5'>ROI selection module 'Volume Edit' is launched</font>  \n",
        ">\n",
        "><font size='5'>'Volume Rendering' is created (but inactive) </font>  \n",
        ">\n",
        "><font size='5'>Continue with manual adjustments and ROI selection</font>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEUuR-7J5qzN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "files_0 = os.listdir(new_path)\n",
        "files = []\n",
        "\n",
        "volume_edit_counter = 1            # Number of volume edits\n",
        "\n",
        "extention = '.nii'             # File extention variable\n",
        "                               # Change it if you want to open another file types\n",
        "\n",
        "for file in files_0:\n",
        "\n",
        "    if file.endswith(extention):\n",
        "        file_path = os.path.join(new_path, file)\n",
        "        some_obj = hx_project.load(file_path)\n",
        "        file = file.replace(\" \", \"-\")\n",
        "        if volume_edit_counter > 1:\n",
        "            files.append(file.replace(\".nii\", f\"({volume_edit_counter}).mask\"))\n",
        "        else:\n",
        "            files.append(file.replace(\".nii\", \".mask\"))\n",
        "        # Volume rendering\n",
        "        volumeRenderingSettings = hx_object_factory.create('HxVolumeRenderingSettings')\n",
        "        volumeRender = hx_object_factory.create('HxVolumeRender2')\n",
        "        hx_project.add(volumeRenderingSettings)\n",
        "        hx_project.add(volumeRender)\n",
        "        volumeRender.ports.volumeRenderingSettings.connect(volumeRenderingSettings)\n",
        "        volumeRenderingSettings.ports.data.connect(hx_project.get(file))\n",
        "        # 3D edges setting in Volume rendering\n",
        "        volumeRenderingSettings.ports.effects.toggles[0].checked = HxPortToggleList.Toggle.CHECKED\n",
        "        volumeRenderingSettings.viewer_mask = False\n",
        "        volumeRenderingSettings.update()\n",
        "        volumeRender.update()\n",
        "        volumeRender.ports.colormap.auto_adjust_range = False\n",
        "        volumeRenderingSettings.ports.data.connect(hx_project.get(file))\n",
        "        volumeRender.update()\n",
        "\n",
        "        for iter in range(volume_edit_counter):\n",
        "            volume_edit= hx_object_factory.create('HxVolumeEdit')\n",
        "            hx_project.add(volume_edit)\n",
        "            print(iter)\n",
        "\n",
        "            if iter == 0:\n",
        "                volume_edit.ports.data.connect(hx_project.get(file))\n",
        "                volume_edit.execute()\n",
        "                file_from_volren_first = file.replace(\".nii\", \".modif\")\n",
        "                print(file_from_volren_first)\n",
        "\n",
        "            else:\n",
        "                volume_edit.ports.data.connect(hx_project.get(file_from_volren_first))\n",
        "                volume_edit.execute()\n",
        "                file_from_volren_first = file.replace(\".nii\", f\"({iter+1}).modif\")\n",
        "                print(file_from_volren_first)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgd6wlFi5qzO"
      },
      "source": [
        "# Manual selection of ROI \n",
        "><font size='5'>1. Select ROI of your choice using **'Cut'** 'Inside/Outside'</font>  \n",
        ">  \n",
        "><font size='5'>2. **'Create mask'** using the 'Volume Edit' tool in Avizo</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcZT_R5J5qzO"
      },
      "source": [
        "# Saving\n",
        "\n",
        "><font size='5'>Paste the following **code** into Avizo console</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgAkUqFD5qzO"
      },
      "outputs": [],
      "source": [
        "for file in files:\n",
        "    # Get volume fraction\n",
        "    get_volume_fraction = hx_object_factory.create('label_ratio')\n",
        "    hx_project.add(get_volume_fraction)\n",
        "    get_volume_fraction.ports.inputImage.connect(hx_project.get(file))\n",
        "    get_volume_fraction.execute()\n",
        "    file_get_volume_fraction = file.replace('.thresholded', '.measure')\n",
        "\n",
        "hx_project.save(os.path.join(new_path, f'{tag}_ROI_autosave.hx'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSNLMDeS5qzO"
      },
      "source": [
        "><font size='5'>Clear 'Project view' in Avizo</font>\n",
        ">  \n",
        "><font size='5'>Clear and restart the Python console</font>\n",
        ">  \n",
        "><font size='5'>Copy the **new_path** and **tag** into the Avizo console again (see beginning of Step 2)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqYHNNlq5qzO"
      },
      "source": [
        "## <font color = 'orangered' size=6>Image processing </font>\n",
        "\n",
        "# <font color = '#32CD32'>3. Process the image and save the results </font>\n",
        " Warning: this step requires computer RAM resources.  \n",
        " We do not recommend to upload stacks of large images.\n",
        "\n",
        "><font size='5'>For images with extentions other than .nii:</font>   \n",
        "><font size='4'>Change the **'extention'** variable</font>   \n",
        "\n",
        "><font size='5'>Paste the following **code** into Avizo console</font>  \n",
        "\n",
        "><font size='5'>Filter settings can be adjusted with respect to your image properties </font>  \n",
        "><font size='4'>See the settings below</font>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0ixUY0J5qzP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "files = []\n",
        "all_ranges = []\n",
        "list_for_newData =[]\n",
        "files_from_unsharpMask = []\n",
        "ranges_for_data = []\n",
        "files_from_threshold = []\n",
        "files_from_spatial_graph_statistics = []\n",
        "\n",
        "#////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "extention = '.modif'            # File extention variable\n",
        "                                # Can be set for various file types: .nii, .modif, .tiff, etc.\n",
        "\n",
        "number_of_images_for_threshold = 3    # Set number of images in the series\n",
        "                                      # (if number_of_images_for_threshold = 0, the threshold will be calculated for each image)\n",
        "\n",
        "first_point_shift = 0.985    # The percentage by which the left autorange limit is moved\n",
        "                             # Calculated as (default_autorange - default_autorange * first_point_shift)\n",
        "\n",
        "second_point_shift = 0.9     # The percentage by which the right autorange limit is moved\n",
        "                             # Calculated as (default_autorange - default_autorange * second_point_shift)\n",
        "\n",
        "multiplier_for_frameshift_of_volren = 1.2  # Range frameshift for the Volume Rendering filter\n",
        "                                         # If the image is too bright, adjust the settings respectively\n",
        "\n",
        "threshold_multiplier = 2.8    # Threshold shift multiplier to avoid noise and mess\n",
        "                              # Removes ranges from ranges frame that a visualized by threshold_multiplier\n",
        "                              # Calculated as:\n",
        "                              # ((first_point_shift/second_point_shift * ranges from autothreshold) * multiplier_for_frameshift_of_volren)) * threshold_multiplier\n",
        "#////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "if extention == '.modif':\n",
        "    new_path = f'{new_path}\\{tag}_ROI_autosave-files'\n",
        "    files_0 = os.listdir(new_path)\n",
        "else:\n",
        "    files_0 = os.listdir(new_path)\n",
        "\n",
        "    # Image upload and title modification\n",
        "for file in files_0:\n",
        "    if file.endswith(extention):\n",
        "        print(file)\n",
        "        file_path = os.path.join(new_path, file)\n",
        "        some_obj = hx_project.load(file_path)\n",
        "        file = file.replace(\" \", \"-\")\n",
        "        files.append(file)\n",
        "\n",
        "            # Hessian filter\n",
        "        hessian_filter = hx_object_factory.create('structureenhancementfilter')\n",
        "        hx_project.add(hessian_filter)\n",
        "        hessian_filter.ports.inputImage.connect(hx_project.get(file))\n",
        "\n",
        "        #//////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "        hessian_filter.ports.interpretation.selected = 1                      # Hessian filter: 3D (or XY) interpretation mode settings\n",
        "        hessian_filter.ports.standardDeviationMinMax.texts[0].value = 0.8     # Hessian filter: standard deviation MIN\n",
        "        hessian_filter.ports.standardDeviationMinMax.texts[1].value = 2.5      # Hessian filter: standard deviation MAX\n",
        "        hessian_filter.ports.standardDeviationStep.texts[0].value = 0.8       # Hessian filter: standard deviation STEP\n",
        "        #//////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "            # Hessian filter: type settings\n",
        "        hessian_filter.ports.structureType.menus[0] = HxPortButtonMenu.Menu(options = ['Rod', 'Ball', 'Plane'], selected = 0)\n",
        "        hessian_filter.update()\n",
        "        hessian_filter.execute()\n",
        "        hessian_filter.fire()\n",
        "        file_from_hessian_filter = file.replace(extention, \".filtered\")\n",
        "\n",
        "            # Unsharp masking filter\n",
        "        unsharpMask = hx_object_factory.create('unsharpmaskfilter')\n",
        "        hx_project.add(unsharpMask)\n",
        "\n",
        "        #//////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "        unsharpMask.ports.interpretation.selected = 0         # Unsharp mask: interpolation type\n",
        "        unsharpMask.ports.edgeSize.texts[0].value = 5         # Unsharp mask: edge size settings\n",
        "        unsharpMask.ports.edgeContrast.texts[0].value = 0.99  # Unsharp mask: edge contrast settings\n",
        "        #//////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "        \n",
        "        unsharpMask.ports.inputImage.connect(hx_project.get(file_from_hessian_filter))\n",
        "        unsharpMask.execute()\n",
        "        unsharpMask.fire()\n",
        "        if file.endswith(extention):\n",
        "            file_from_unsharpMask = file.replace(extention, \"(2).filtered\")\n",
        "        else:\n",
        "            assert 'Please rename your file'\n",
        "        files_from_unsharpMask.append(file_from_unsharpMask)\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "# Volume rendering\n",
        "volumeRenderingSettings = hx_object_factory.create('HxVolumeRenderingSettings')\n",
        "volumeRender = hx_object_factory.create('HxVolumeRender2')\n",
        "hx_project.add(volumeRenderingSettings)\n",
        "hx_project.add(volumeRender)\n",
        "volumeRender.ports.volumeRenderingSettings.connect(volumeRenderingSettings)\n",
        "volumeRenderingSettings.ports.data.connect(hx_project.get(file_from_unsharpMask))\n",
        "# Volume rendering: 3D edge settings\n",
        "volumeRenderingSettings.ports.effects.toggles[0].checked = HxPortToggleList.Toggle.CHECKED\n",
        "\n",
        "# Autorange collection\n",
        "for file in files_from_unsharpMask:\n",
        "    ranges = []\n",
        "    volumeRenderingSettings.fire()\n",
        "    volumeRender.ports.colormap.auto_adjust_range = True\n",
        "    volumeRenderingSettings.ports.data.connect(hx_project.get(file))\n",
        "    volumeRender.update()\n",
        "\n",
        "    if number_of_images_for_threshold > 0:\n",
        "        if 'thr' in file:\n",
        "            range_for_spec = volumeRender.ports.colormap.range\n",
        "            # Autorange recalculation\n",
        "            ranges.append(range_for_spec[0] - range_for_spec[0] * first_point_shift)\n",
        "            ranges.append(range_for_spec[1] - range_for_spec[1] * second_point_shift)\n",
        "            for iter in range(number_of_images_for_threshold):\n",
        "                all_ranges.append(ranges)\n",
        "\n",
        "    elif number_of_images_for_threshold == 0:\n",
        "        range_for_spec = volumeRender.ports.colormap.range\n",
        "        # Autorange recalculation\n",
        "        if range_for_spec[0] - range_for_spec[0] * first_point_shift == 0:\n",
        "            ranges.append((range_for_spec[1] - range_for_spec[1] * first_point_shift)/10)\n",
        "        else:\n",
        "            ranges.append(range_for_spec[0] - range_for_spec[0] * first_point_shift)\n",
        "        ranges.append(range_for_spec[1] - range_for_spec[1] * second_point_shift)\n",
        "        all_ranges.append(ranges)\n",
        "\n",
        "# Prints the list of shifted autoranges\n",
        "print('\\n All shifted ranges: \\n ___________________________________ \\n', all_ranges)\n",
        "\n",
        "_ = 0\n",
        "# Volume Rendering: jumping settings\n",
        "for file in files_from_unsharpMask:\n",
        "    volumeRenderingSettings.fire()\n",
        "    volumeRender.ports.colormap.auto_adjust_range = False\n",
        "    volumeRender.update()\n",
        "    volumeRenderingSettings.ports.data.connect(hx_project.get(file))\n",
        "\n",
        "    # Range percentage multiplier\n",
        "    range_for_volren = [element * multiplier_for_frameshift_of_volren for element in all_ranges[_]]\n",
        "    ranges_for_data.append(range_for_volren)\n",
        "    print(f'\\n Range for correct volume rendering [{_}]: \\n ___________________________________ \\n' , range_for_volren)\n",
        "    volumeRender.ports.colormap.range = range_for_volren\n",
        "    volumeRender.ports.alphaScale.value = 0.4              # Transparency settings\n",
        "    volumeRenderingSettings.ports.rendering.selected = 0   # Standard type rendering settings\n",
        "    volumeRenderingSettings.fire()\n",
        "    _+=1\n",
        "\n",
        "_ = 0\n",
        "# Interactive threshold\n",
        "for file_from_unsharpMask in files_from_unsharpMask:\n",
        "\n",
        "    # Interactive threshold: recalculation\n",
        "    range_for_threshold =  [element * threshold_multiplier for element in ranges_for_data[_]]\n",
        "    print(f'\\n Range for thresholding [{_}]: \\n ___________________________________ \\n', range_for_threshold)\n",
        "    interactive_threshold = hx_object_factory.create('HxInteractiveThreshold')\n",
        "    hx_project.add(interactive_threshold)\n",
        "    interactive_threshold.ports.data.connect(hx_project.get(file_from_unsharpMask))\n",
        "\n",
        "    # Interactive threshold: 3D preview type settings\n",
        "    interactive_threshold.ports.preview.toggles[0].checked = HxPortToggleList.Toggle.UNCHECKED\n",
        "    interactive_threshold.ports.preview.toggles[1].checked = HxPortToggleList.Toggle.CHECKED\n",
        "    interactive_threshold.update()\n",
        "    interactive_threshold.ports.intensityRange.range = range_for_threshold\n",
        "    interactive_threshold.update()\n",
        "    interactive_threshold.apply_transform_to_result = False\n",
        "    interactive_threshold.execute()\n",
        "    interactive_threshold.fire()\n",
        "    _+=1\n",
        "    file_from_threshold = file_from_unsharpMask.replace(\".filtered\", \".thresholded\")\n",
        "    files_from_threshold.append(file_from_threshold)\n",
        "    interactive_threshold.viewer_mask = False\n",
        "    interactive_threshold.update()\n",
        "    test_export = hx_project.get(file_from_threshold)\n",
        "\n",
        "for file in files_from_threshold:\n",
        "\n",
        "    # Pruning: filtering for thresholded binary images\n",
        "    pruning= hx_object_factory.create('pruning3d')\n",
        "    hx_project.add(pruning)\n",
        "    pruning.ports.inputBinaryImage.connect(hx_project.get(file))\n",
        "    pruning.ports.numberOfIterations.texts[0].value = 10        # Pruning: number of iterations\n",
        "    pruning.execute()\n",
        "    pruning.fire()\n",
        "    file_from_pruning = file.replace('.thresholded', '.pruned')\n",
        "\n",
        "    # Collection of volume fraction\n",
        "    get_volume_fraction = hx_object_factory.create('label_ratio')\n",
        "    hx_project.add(get_volume_fraction)\n",
        "    get_volume_fraction.ports.inputImage.connect(hx_project.get(file_from_pruning))\n",
        "    get_volume_fraction.execute()\n",
        "    file_get_volume_fraction = file.replace('.thresholded', '.measure')\n",
        "\n",
        "    # Centerline tree (for thresholded binary images)\n",
        "    centrline_tree = hx_object_factory.create('HxTEASAR')\n",
        "    hx_project.add(centrline_tree)\n",
        "    centrline_tree.ports.data.connect(hx_project.get(file_from_pruning))\n",
        "    centrline_tree.ports.tubesParams.texts[0].value = 2        # Centrline tree Slope\n",
        "    centrline_tree.ports.tubesParams.texts[1].value = 4        # Centrline tree ZeroVal\n",
        "    centrline_tree.update()\n",
        "    centrline_tree.execute()\n",
        "    centrline_tree.fire()\n",
        "    file_centrline_tree = file.replace('.thresholded', '.Spatial-Graph')\n",
        "\n",
        "    # Collecting statistics from Centerline tree using Spartial Graph Statistics\n",
        "    spatial_graph_statistics = hx_object_factory.create('HxSpatialGraphStats')\n",
        "    hx_project.add(spatial_graph_statistics)\n",
        "    spatial_graph_statistics.ports.data.connect(hx_project.get(file_centrline_tree))\n",
        "    # Attribute graphs and Statistics: Toggle settings\n",
        "    spatial_graph_statistics.ports.output.toggles[0].checked = HxPortToggleList.Toggle.CHECKED\n",
        "    spatial_graph_statistics.ports.output.toggles[1].checked = HxPortToggleList.Toggle.CHECKED\n",
        "    spatial_graph_statistics.execute()\n",
        "    spatial_graph_statistics.fire()\n",
        "    file_from_spatial_graph_statistics = file.replace('.thresholded', '.attributegraph')\n",
        "    files_from_spatial_graph_statistics.append(file_from_spatial_graph_statistics)\n",
        "\n",
        "# Autosaving your project in Avizo\n",
        "if extention == '.modif':\n",
        "    hx_project.save(os.path.join(os.path.dirname(new_path), f'{tag}_autosave.hx'))\n",
        "\n",
        "else:\n",
        "    hx_project.save(os.path.join(new_path, f'{tag}_autosave.hx'))\n",
        "\n",
        "# Export of Attributegraphs from Spatial Graph Statistics as .XML files\n",
        "for file_from_spatial_graph_statistics in files_from_spatial_graph_statistics:\n",
        "\n",
        "    out = hx_project.get(file_from_spatial_graph_statistics)\n",
        "    out.update()\n",
        "    out.update()\n",
        "    out.ports.show.buttons[1].hit = True\n",
        "    out.update()\n",
        "    out.ports.show.buttons[1].hit = False\n",
        "    out.update()\n",
        "    out.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65dUKUkk5qzQ"
      },
      "source": [
        "><font size='5'>Press 'Autosave' in Avizo to save project  </font>\n",
        ">\n",
        "><font size='5'>Save .XML files to the folder with images</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFS44ZQz5qzR"
      },
      "source": [
        "# <font color = 'orangered' size=6>Output Data processing </font>   \n",
        "\n",
        "# <font color = '#32CD32'>4. Script for reading and filtering (denoising) the .XML output from Avizo</font>\n",
        "><font size = 5>Set 'file_path' to a folder with .XML data  </font>  \n",
        ">\n",
        "><font size = 5>Define the 'pattern' variable to get an appropriate title for 'Condition' labeling </font>  \n",
        ">\n",
        "><font size = 5>Define the 'selector' and select the column name to apply a threshold to remove the noise from the data.</font>\n",
        ">\n",
        "><font size = 5>Define 'threshold' variable for better results  </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fYj6aEb45qzR"
      },
      "outputs": [],
      "source": [
        "file_path = rf'C:\\Folder_with_images\\Scaled\\Mouse'   # Path to the folder with preprocessed images\n",
        "\n",
        "tag = \"Mouse\"\n",
        "\n",
        "is_ROI_selected = 'Yes'                 # (Yes/No) If we are using ROI in our images\n",
        "\n",
        "pattern = r'^([a-zA-Z0-9_-]+).*'       # Select pattern for 'Condition' titling\n",
        "\n",
        "selector = 'ChordLength'               # Select column name to apply the threshold for noise filtering\n",
        "                                       # Use ChordLength' or 'CurvedLength' for the best result\n",
        "threshold =  40000                     # Drop all the values outside the threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "><font size = 5>Run the functions below</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_values_from_file(autosaved_directory_path):\n",
        "\n",
        "    ''' Extraction and converting of volume values and ratios to dataframe format '''\n",
        "\n",
        "    name = ''\n",
        "    value_2 = None\n",
        "    value_3 = None\n",
        "    value_4 = None\n",
        "    value_5 = None\n",
        "    value_6 = None\n",
        "\n",
        "    # Define regular expressions to match the desired patterns\n",
        "    name_pattern = r'Filename \"(.*?)\"'\n",
        "    value_pattern = r'@(\\d+)\\s+([\\d.e+-]+)'\n",
        "\n",
        "    # Open and read the file\n",
        "    with open(autosaved_directory_path, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "        # Extract the name\n",
        "        name_match = re.search(name_pattern, content)\n",
        "        if name_match:\n",
        "            name = name_match.group(1)\n",
        "\n",
        "        # Extract values using regular expressions\n",
        "        value_matches = re.finditer(value_pattern, content)\n",
        "        for match in value_matches:\n",
        "            key = match.group(1)\n",
        "            val = match.group(2)\n",
        "            if key == '2':\n",
        "                value_2 = float(val)\n",
        "            elif key == '3':\n",
        "                value_3 = float(val)\n",
        "            elif key == '4':\n",
        "                value_4 = float(val)\n",
        "            elif key == '5':\n",
        "                value_5 = float(val)\n",
        "            elif key == '6':\n",
        "                value_6 = float(val)\n",
        "\n",
        "    return {\n",
        "        'Filename': os.path.basename(name),\n",
        "        'Volume Fraction': value_2,\n",
        "        'Label Volume': value_3,\n",
        "        'Total Volume': value_4,\n",
        "        'Label Voxel Count': value_5,\n",
        "        'Total Voxel Count': value_6\n",
        "    }\n",
        "\n",
        "def get_segments_sheet(file):\n",
        "    \"\"\"\n",
        "        A function to process .XML file, and then create and return a pandas DataFrame.\n",
        "        It loads the XML file and loads the 'Segments' sheet from the three output sheets of an .xml file from Avizo.\n",
        "        The function is iterated over the rows in the sheet and over the cells in the row to get the value of each cell.\n",
        "        Return pandas DataFrame.\n",
        "    \"\"\"\n",
        "    tree = ET.parse(file)\n",
        "    root = tree.getroot()\n",
        "    segments_sheet = root.find(\".//ss:Worksheet[@ss:Name='Segments']\", namespace_map)\n",
        "\n",
        "    data_rows = []\n",
        "    for row in segments_sheet.findall('.//ss:Row', namespace_map):\n",
        "        data = []\n",
        "        for cell in row.findall('.//ss:Cell', namespace_map):\n",
        "            data.append(cell.find('.//ss:Data', namespace_map).text)\n",
        "        data_rows.append(data)\n",
        "\n",
        "    df = pd.DataFrame(data_rows)\n",
        "    df.columns = df.iloc[0]\n",
        "    df = df[1:]\n",
        "\n",
        "    return df\n",
        "\n",
        "def get_nodes_sheet(file):\n",
        "    \"\"\"\n",
        "        A function to process .XML file, and then create and return a pandas DataFrame.\n",
        "        It loads the XML file and loads the 'Nodes' sheet from the three output sheets of an .xml file from Avizo.\n",
        "        The function is iterated over the rows in the sheet and over the cells in the row to get the value of each cell.\n",
        "        Return pandas DataFrame.\n",
        "    \"\"\"\n",
        "    tree_nodes = ET.parse(file)\n",
        "    root_nodes = tree_nodes.getroot()\n",
        "    segments_sheet_nodes = root_nodes.find(\".//ss:Worksheet[@ss:Name='Nodes']\", namespace_map)\n",
        "\n",
        "    data_rows_nodes = []\n",
        "    for row in segments_sheet_nodes.findall('.//ss:Row', namespace_map):\n",
        "        data_nodes = []\n",
        "        for cell in row.findall('.//ss:Cell', namespace_map):\n",
        "            data_nodes.append(cell.find('.//ss:Data', namespace_map).text)\n",
        "        data_rows_nodes.append(data_nodes)\n",
        "\n",
        "    df_nodes = pd.DataFrame(data_rows_nodes)\n",
        "    df_nodes.columns = df_nodes.iloc[0]\n",
        "    df_nodes = df_nodes[1:]\n",
        "\n",
        "    return df_nodes\n",
        "\n",
        "def count_symbols(df):\n",
        "    \"\"\"\n",
        "    Function to count the values\n",
        "    \"\"\"\n",
        "    symbol_counts = df['Coordination Number'].value_counts()\n",
        "    symbol_1_count = 0\n",
        "    other_symbols_count = 0\n",
        "    max_symbol_value = 0\n",
        "\n",
        "    for symbol, count in symbol_counts.items():\n",
        "        if int(symbol) == 1:\n",
        "            symbol_1_count = count\n",
        "        else:\n",
        "            other_symbols_count += count\n",
        "\n",
        "        if int(symbol) > int(max_symbol_value):\n",
        "            max_symbol_value = symbol\n",
        "\n",
        "    return symbol_1_count, other_symbols_count, max_symbol_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "namespace_map = { 'ss': 'urn:schemas-microsoft-com:office:spreadsheet' } # Namescape parameter for reading the Avizo output.xml\n",
        "\n",
        "data = []\n",
        "ROI_data = []\n",
        "\n",
        "if is_ROI_selected == 'Yes':\n",
        "    ROI_directory_path = f'{file_path}\\{tag}_ROI_autosave-files'\n",
        "    Vessels_directory_path = f'{file_path}\\{tag}_autosave-files'\n",
        "    print('We work with ROI')\n",
        "elif is_ROI_selected == 'No':\n",
        "    Vessels_directory_path = f'{file_path}\\{tag}_autosave-files'\n",
        "    ROI_directory_path = f'{file_path}\\{tag}_autosave-files'\n",
        "    print('We work without ROI')\n",
        "\n",
        "if os.path.exists(ROI_directory_path) and os.path.isdir(ROI_directory_path):\n",
        "    list_of_ROI = os.listdir(ROI_directory_path)\n",
        "    if len(list_of_ROI) > 0:\n",
        "        for filename in os.listdir(ROI_directory_path):\n",
        "            if filename.endswith('.measure'):\n",
        "                autosaved_ROI_directory_path = os.path.join(ROI_directory_path, filename)\n",
        "                ROI_extracted_data = extract_values_from_file(autosaved_ROI_directory_path)\n",
        "                ROI_data.append(ROI_extracted_data)\n",
        "    else:\n",
        "        print(ROI_directory_path)\n",
        "        print(\"Warning: No files with ROI in folder.\")\n",
        "else:\n",
        "    print(ROI_directory_path)\n",
        "    print(\"Warning: folder with ROI does not exist.\")\n",
        "\n",
        "for filename in os.listdir(Vessels_directory_path):\n",
        "    if filename.endswith('.measure'):\n",
        "        autosaved_directory_path = os.path.join(Vessels_directory_path, filename)\n",
        "        extracted_data = extract_values_from_file(autosaved_directory_path)\n",
        "        data.append(extracted_data)\n",
        "\n",
        "Vessels_volume_relations = pd.DataFrame(data)\n",
        "ROI_volume_relations = pd.DataFrame(ROI_data)\n",
        "\n",
        "dataframes = []                     # List to store DataFrames for each file\n",
        "dataframes_names = []               # List to store DataFrames name for each file\n",
        "processed_dataframes = []           # List to store processed DataFrames\n",
        "\n",
        "for file in os.listdir(file_path):\n",
        "    if file.endswith(\".xml\"):                               # Ensures that the file is an .XML file\n",
        "        full_path = os.path.join(file_path, file)           # Shows the full file path\n",
        "        df = get_segments_sheet(full_path)                  # Processes the .XML file and creates a DataFrame\n",
        "        df_nodes = get_nodes_sheet(full_path)\n",
        "\n",
        "        # Assuming you have loaded the dataset into a pandas DataFrame called df_nodes\n",
        "        symbol_1_count, other_symbols_count, max_symbol_value = count_symbols(df_nodes)\n",
        "        match = re.match(pattern, file)\n",
        "\n",
        "        df['Number of empty endpoints'] = symbol_1_count\n",
        "        df['Number of branchpoints'] = other_symbols_count\n",
        "        df['Max Coordination Number'] = max_symbol_value\n",
        "\n",
        "        if match:\n",
        "            extracted_text = match.group(1)\n",
        "        df = df.rename(columns={'Segment ID': f'{extracted_text}'})\n",
        "        dataframes.append(df)\n",
        "        dataframes_names.append(file)\n",
        "\n",
        "        # Specification of the threshold for DataFrames\n",
        "        # Filtering of the DataFrames in accordance with the threshold variable\n",
        "i=0\n",
        "for df in dataframes:\n",
        "\n",
        "    if selector in df.columns:                          # Ensures that the 'selector' variable exists in columns\n",
        "        df[selector] = pd.to_numeric(df[selector])      # Converts 'ChordLength' column to numeric\n",
        "        mean_length = df[selector].mean()               # Calculates the mean of column values\n",
        "        dropped_count = len(df[df[selector] < threshold])\n",
        "\n",
        "        df['Number of empty endpoints'] = df['Number of empty endpoints'] - dropped_count * 2\n",
        "        df['Dropped segments'] = dropped_count\n",
        "        df = df[df[selector] >= threshold]              # To refresh the dataset\n",
        "\n",
        "        if is_ROI_selected == 'Yes':\n",
        "            vessels_fraction =  Vessels_volume_relations['Volume Fraction'][i]\n",
        "            ROI_fraction = ROI_volume_relations['Volume Fraction'][i]\n",
        "        elif is_ROI_selected == 'No':\n",
        "            vessels_fraction =  Vessels_volume_relations['Volume Fraction'][i]\n",
        "            ROI_fraction = Vessels_volume_relations['Volume Fraction'][i]\n",
        "        print(\n",
        "              f'______________________________________________',\n",
        "              f'\\n{dataframes_names[i]} - {i+1}',\n",
        "              f'\\nRelation of ROI to whole image = {round(ROI_fraction, 6)}          Ratio of Vessels Mask to background = {round(vessels_fraction, 6)}',\n",
        "              f'\\nMean of {selector} = {round(mean_length, 2)};',\n",
        "              f'   Threshold of {selector} = {threshold};       Dropped = {dropped_count} samples',\n",
        "              )\n",
        "        processed_dataframes.append(df)\n",
        "\n",
        "    else:\n",
        "        print('Column with provided name does not exist')\n",
        "\n",
        "    for column in processed_dataframes[i].columns:\n",
        "        if processed_dataframes[i][column].dtype == 'object':\n",
        "            try:\n",
        "                processed_dataframes[i][column] = processed_dataframes[i][column].astype(float)\n",
        "            except ValueError:\n",
        "                if column == 'Point IDs':\n",
        "                    print()\n",
        "                else:\n",
        "                    print(f\"Column in {dataframes_names[i]} with name '{column}' couldn't be converted to float64\")\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaQCe1525qzT"
      },
      "source": [
        "# <font color = '#32CD32'>5. Creation of datasets with a defined order</font>\n",
        "\n",
        "><font size = 5>Define the dataset order for merging     </font>  \n",
        "(e.g. if there are 4 datasets that need to be splitted into two groups (based on filename), specify 2, 2 in **merging_order**\n",
        "\n",
        "><font size = 5>Merging allows to create dataframes that are specifically suited for your sampling conditions   </font>\n",
        "    \n",
        "><font size = 5>One of the outputs is a merged DataFrame that contains a column named 'conditions' with the filenames of all the images that were analyzed </font>\n",
        "\n",
        "><font size = 5>The other one is 'mean' dataset according to the 'merging order' and mode </font>  \n",
        ">This dataset we save in .xlsx format, that are easy to analyse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0OLKtrb-5qzT"
      },
      "outputs": [],
      "source": [
        "# Define the merging order\n",
        "merging_order = [1]*3\n",
        "# Save frame of Mean values according to path\n",
        "output_path_for_frame_saving = f'{file_path}\\{tag}.xlsx'\n",
        "pd.options.display.max_columns = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oz_L62vF5qzU"
      },
      "outputs": [],
      "source": [
        "dataframes = processed_dataframes\n",
        "merged_dataframes = []\n",
        "start_index = 0\n",
        "for group_size in merging_order:\n",
        "    end_index = start_index + group_size\n",
        "    merged_df = pd.concat(dataframes[start_index:end_index])\n",
        "    merged_dataframes.append(merged_df)\n",
        "    start_index = end_index\n",
        "for i, merged_df in enumerate(merged_dataframes):\n",
        "    merged_df = merged_df.reset_index(drop=True)\n",
        "    merged_dataframes[i] = merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fpr6p2yM5qzU",
        "outputId": "a12cb208-23fe-4264-8c3c-08f4a30f1223"
      },
      "outputs": [],
      "source": [
        "one_big_merged_frame = pd.DataFrame()\n",
        "frame_for_export = pd.DataFrame()\n",
        "labels = []\n",
        "mean_frames = []\n",
        "\n",
        "for dataset in merged_dataframes:\n",
        "    condition_name = dataset.columns[0]\n",
        "    dataset['Condition'] = condition_name\n",
        "    labels.append(condition_name)\n",
        "\n",
        "    dataset['CurvedLength'] = dataset['CurvedLength'] / 1000    # in [um]\n",
        "    dataset['ChordLength'] = dataset['ChordLength'] / 1000      # in [um]\n",
        "    dataset['MeanRadius'] = dataset['MeanRadius'] / 1000        # in [um]\n",
        "    dataset['Volume'] = dataset['Volume'] / 10**9               # in [um^3]\n",
        "\n",
        "    dataset['Volume_Sum_perImage [um^3]'] = dataset['Volume'].sum()\n",
        "\n",
        "    dataset['Geometrical_Volume_perImage [um^3]'] = np.sum((dataset['MeanRadius'])**2 * pi * dataset['CurvedLength']) \n",
        "\n",
        "    dataset['Imaging_Error_perImage'] = 1 - dataset['Geometrical_Volume_perImage [um^3]'] / dataset['Volume_Sum_perImage [um^3]']\n",
        "\n",
        "    curvedlength_sum = dataset['CurvedLength'].sum()\n",
        "\n",
        "    dataset['Vessels_CurvedLength_Sum_perImage [um]'] = curvedlength_sum \n",
        "\n",
        "    dataset['Weighted_MeanRadius_perImage [um]'] = np.sum(dataset['MeanRadius'] * dataset['CurvedLength']) / (curvedlength_sum)\n",
        "\n",
        "    dataset['Weighted_MeanTortuosity_perImage'] = np.sum(dataset['Tortuosity'] * dataset['CurvedLength']) / (curvedlength_sum)\n",
        "\n",
        "    dataset['Weighted_Segment_MeanRadius_perSegment'] = (dataset['MeanRadius'] * dataset['CurvedLength']) / (curvedlength_sum)\n",
        "\n",
        "    dataset['Weighted_Segment_Volume_perSegment'] = (dataset['Volume'] * dataset['CurvedLength']) / (curvedlength_sum)\n",
        "    \n",
        "    selected_columns = (\n",
        "                        dataset.columns[1:22].tolist() + ['Condition'] + ['Volume_Sum_perImage [um^3]'] +\n",
        "                        ['Geometrical_Volume_perImage [um^3]'] + ['Imaging_Error_perImage'] +\n",
        "                        ['Vessels_CurvedLength_Sum_perImage [um]'] + ['Weighted_MeanRadius_perImage [um]'] +\n",
        "                        ['Weighted_MeanTortuosity_perImage'] + ['Weighted_Segment_MeanRadius_perSegment'] \n",
        "                        + ['Weighted_Segment_Volume_perSegment'] \n",
        "                        )\n",
        "\n",
        "    dataset = dataset[selected_columns].copy()\n",
        "    one_big_merged_frame = pd.concat([one_big_merged_frame, dataset], ignore_index=True)\n",
        "    dataset.loc[:, 'Weighted_Segment_MeanRadius_perSegment'] = sum(dataset['Weighted_Segment_MeanRadius_perSegment'])\n",
        "    dataset.loc[:, 'Weighted_Segment_Volume_perSegment'] = sum(dataset['Weighted_Segment_Volume_perSegment'])\n",
        "    means = dataset.mean(skipna=True, numeric_only=True)\n",
        "    mean_df = pd.DataFrame(means, columns=[f'{condition_name}'])\n",
        "    \n",
        "    mean_df = mean_df.T\n",
        "    mean_frames.append(mean_df)\n",
        "\n",
        "frame_for_export = pd.concat(mean_frames, axis=0)\n",
        "frame_for_export['Filename'] = frame_for_export.index\n",
        "frame_for_export.reset_index(drop=True, inplace=True)\n",
        "frame_for_export.insert(0, 'Filename', frame_for_export.pop('Filename'))\n",
        "frame_for_export['Total Image Volume [um^3]'] = Vessels_volume_relations['Total Volume'] \n",
        "frame_for_export['ROI Volume [um^3]'] = ROI_volume_relations['Label Volume'] \n",
        "frame_for_export['Vessels in ROI Volume (Labeled Volume after pruning before denoizing) [um^3]'] = Vessels_volume_relations['Label Volume'] \n",
        "frame_for_export['Ratio of ROI Mask to whole image'] = ROI_volume_relations['Volume Fraction'] \n",
        "frame_for_export['Ratio of Vessel Mask to whole image'] = Vessels_volume_relations['Volume Fraction'] \n",
        "division_result = frame_for_export['Vessels in ROI Volume (Labeled Volume after pruning before denoizing) [um^3]'] / frame_for_export['ROI Volume [um^3]']\n",
        "frame_for_export['Ratio of Vessel Mask to background ROI'] = division_result\n",
        "columns_to_drop = [col for col in frame_for_export.columns if col.startswith('Tensor') or col.startswith('Node') or col.startswith('Subgraph')]\n",
        "frame_for_export.drop(columns=columns_to_drop, inplace=True)\n",
        "writer = pd.ExcelWriter(output_path_for_frame_saving, engine='xlsxwriter')\n",
        "frame_for_export.to_excel(writer, sheet_name='Sheet1', index=True)\n",
        "workbook = writer.book\n",
        "worksheet = writer.sheets['Sheet1']\n",
        "\n",
        "# Set the width of Column 1\n",
        "worksheet.set_column('A:A', 4)\n",
        "worksheet.set_column('B:S', 13)\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nwIET975qzV"
      },
      "source": [
        "<font size='5' color = '#0E4C92'>Output dataframes:</font>\n",
        "\n",
        "><font size='5'>frame_for_export</font>\n",
        "\n",
        "Consists of 'mean' by dataframe observation.  \n",
        "Each observation defined by sample group was provided in merging.\n",
        "  \n",
        "><font size='5'>one_big_merged_frame</font>\n",
        "\n",
        "Consists of all dataframes with column 'Condition', where the sample names are presented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o20RSZV5qzV"
      },
      "source": [
        "# <font color = 'orangered' size=6>Output representation </font>   \n",
        "\n",
        "# <font color = '#32CD32'>6. Boxplots for all samples with respective stats and annotations</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "><font size='5'>Here we work with 'one_big_merged_frame' dataset\n",
        "\n",
        "Makes plots according to 'Condition' column and our groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "col_name = 'CurvedLength'             # col_name = 'column_name'\n",
        "                                    # Fill it with the name of the column you want to analyse\n",
        "\n",
        "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
        "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
        "fig, axes = plt.subplots(figsize=(6, 6), dpi=100)\n",
        "sns.boxplot(data=one_big_merged_frame, x='Condition', y=col_name, hue='Condition', palette='pastel', ax=axes, showfliers=0, dodge=False)\n",
        "axes.set_xticklabels(axes.get_xticklabels(), rotation = -10)\n",
        "legend = plt.legend( loc='upper left', bbox_to_anchor=(0.9, 1))\n",
        "plt.title(col_name, pad = 20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SETS PER SEGMENT  \n",
        "  \n",
        "><font size='5'>Choose the feature set and run the selected cell</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SET 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "col_names_to_analyze_perSegment = ['Tortuosity', \n",
        "                                   'MeanRadius',\n",
        "                                   'ChordLength', \n",
        "                                   'CurvedLength']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SET 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "col_names_to_analyze_perSegment = ['Volume', \n",
        "                                   'Weighted_Segment_MeanRadius_perSegment',\n",
        "                                   'Weighted_Segment_Volume_perSegment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "><font size='5'>If you prefer the representation of your data in log scale - run the cell below</font>  \n",
        "  \n",
        "><font size='5'>Otherwise skip it</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in col_names_to_analyze_perSegment:\n",
        "    one_big_merged_frame[col] = np.log10(one_big_merged_frame[col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# perSegment features plotting (of the preselected 'SET #')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_subplots = len(col_names_to_analyze_perSegment)\n",
        "fig, axes = plt.subplots(1, num_subplots, figsize=(num_subplots * 5, 7), dpi=750)\n",
        "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
        "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
        "legend_handles = []\n",
        "for i, col_name in enumerate(col_names_to_analyze_perSegment):\n",
        "    ax = axes[i]\n",
        "    sns.violinplot(data=one_big_merged_frame, x='Condition', y=col_name, hue='Condition', palette='pastel', ax=ax, showfliers=1, dodge=False)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=-90)\n",
        "    ax.set_title(col_name, pad=30)\n",
        "    ax.get_legend().remove()\n",
        "    ax.set_xlabel('')\n",
        "\n",
        "    if i == len(col_names_to_analyze_perSegment) - 1:\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        legend_handles.extend(handles)\n",
        "legend = plt.legend(legend_handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SETS PER IMAGE  \n",
        "  \n",
        "><font size='5'>Choose the feature set and run the selected cell</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SET 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "col_names_to_analyze_perImage = ['Dropped segments',\n",
        "                                 'Number of empty endpoints',\n",
        "                                 'Number of branchpoints',\n",
        "                    \t         'Volume_Sum_perImage [um^3]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SET 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "col_names_to_analyze_perImage = ['Geometrical_Volume_perImage [um^3]',\n",
        "                                 'Imaging_Error_perImage',\n",
        "                                 'Vessels_CurvedLength_Sum_perImage [um]',\n",
        "                                 'Weighted_MeanRadius_perImage [um]',\n",
        "                                 'Weighted_MeanTortuosity_perImage']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# perImage features plotting (of the preselected 'SET #')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_subplots = len(col_names_to_analyze_perImage)\n",
        "\n",
        "fig, axes = plt.subplots(1, num_subplots, figsize=(num_subplots * 6, 6), dpi=750)\n",
        "custom_params = {\n",
        "    \"axes.spines.right\": False,\n",
        "    \"axes.spines.top\": False,\n",
        "    \"axes.labelsize\": 16,  \n",
        "    \"axes.titlesize\": 18,\n",
        "    \"xtick.labelsize\": 16,\n",
        "    \"ytick.labelsize\": 16,\n",
        "    \"legend.fontsize\": 16\n",
        "}\n",
        "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
        "\n",
        "for i, col_name in enumerate(col_names_to_analyze_perImage):\n",
        "    ax = axes[i]\n",
        "    sns.boxplot(data=one_big_merged_frame, x='Condition', y=col_name, hue='Condition', palette='pastel', ax=ax, showfliers=1, dodge=False)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=-90)\n",
        "    ax.set_title(col_name, pad=30)\n",
        "    ax.get_legend().remove()\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('')\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMJOQX-d5qzV"
      },
      "source": [
        "# <font color = '#32CD32'>7. Statistical tests</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "><font size = 5>Choose above and run the cell with \"SET #\" of data (perImage or perSegment) you want to analyse</font>  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kruskall test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "col_names_to_analyze = col_names_to_analyze_perImage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature for test: Geometrical_Volume_perImage [um^3]\n",
            "Feature for test: Imaging_Error_perImage\n",
            "Feature for test: Vessels_CurvedLength_Sum_perImage [um]\n",
            "Feature for test: Weighted_MeanRadius_perImage [um]\n",
            "Feature for test: Weighted_MeanTortuosity_perImage\n"
          ]
        }
      ],
      "source": [
        "ks_statistic_total = []\n",
        "ks_p_value_total = []\n",
        "for col in col_names_to_analyze:\n",
        "    print('Feature for test:', col)\n",
        "    # Kolmogorov-Smirnov test\n",
        "    conditions = one_big_merged_frame['Condition'].unique()\n",
        "    box_pairs = list(itertools.combinations(conditions, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_p_value_interval(p_value):\n",
        "    p_value_legend = {\n",
        "        'ns': (5.00e-02 < p_value <= 1.00e+00),\n",
        "        '*': (1.00e-02 < p_value <= 5.00e-02),\n",
        "        '**': (1.00e-03 < p_value <= 1.00e-02),\n",
        "        '***': (1.00e-04 < p_value <= 1.00e-03),\n",
        "        '****': (p_value <= 1.00e-04)\n",
        "    }\n",
        "    for key, interval_check in p_value_legend.items():\n",
        "        if interval_check:\n",
        "            return key\n",
        "    return None\n",
        "\n",
        "num_subplots = len(col_names_to_analyze)\n",
        "fig, axes = plt.subplots(1, num_subplots, figsize=(num_subplots * 4, 7), dpi=750)\n",
        "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
        "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
        "legend_handles = []\n",
        "\n",
        "for i, col_name in enumerate(col_names_to_analyze):\n",
        "    ax = axes[i]\n",
        "    sns.boxplot(data=one_big_merged_frame, x='Condition', y=col_name, hue='Condition', palette='pastel', ax=ax, showfliers=1, dodge=False)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=-90)\n",
        "    ax.set_title(col_name, pad=90)\n",
        "    ks_number = len(one_big_merged_frame['Condition'].unique())\n",
        "    sequence_of_data = list(range(1, ks_number+1))\n",
        "    sequence_of_data_pairs = list(itertools.combinations(sequence_of_data, 2))\n",
        "\n",
        "    ax.get_legend().remove()\n",
        "    ax.set_xlabel('')\n",
        "    if i == len(col_names_to_analyze) - 1:\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        legend_handles.extend(handles)\n",
        "    \n",
        "    test_results = add_stat_annotation(ax, data=one_big_merged_frame, x='Condition', y=col_name, \n",
        "                                       box_pairs=box_pairs, test='Kruskal', text_format='star', \n",
        "                                       loc='outside', verbose=2)\n",
        "\n",
        "legend = plt.legend(legend_handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KS test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "col_names_to_analyze = col_names_to_analyze_perSegment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ks_statistic_total = []\n",
        "ks_p_value_total = []\n",
        "for col in col_names_to_analyze:\n",
        "    print('Feature for test:', col)\n",
        "    # Kolmogorov-Smirnov test\n",
        "    conditions = one_big_merged_frame['Condition'].unique()\n",
        "    box_pairs = list(itertools.combinations(conditions, 2))\n",
        "    for pair in box_pairs:\n",
        "        data1 = one_big_merged_frame[one_big_merged_frame.Condition == pair[0]][col]\n",
        "        data2 = one_big_merged_frame[one_big_merged_frame.Condition == pair[1]][col]  \n",
        "        ks_result = scipy.stats.ks_2samp(data1, data2)\n",
        "        ks_statistic, p_value = scipy.stats.ks_2samp(data1, data2)\n",
        "        ks_statistic_total.append(ks_statistic)\n",
        "        ks_p_value_total.append(p_value)\n",
        "        print(f'Samples pair:', pair, 'KS result:', ks_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "><font size = 5>Type in the pairs for KS test</font>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequence_of_data_pairs = [(1,4), (2,5), (3,6)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_p_value_interval(p_value):\n",
        "    p_value_legend = {\n",
        "        'ns': (5.00e-02 < p_value <= 1.00e+00),\n",
        "        '*': (1.00e-02 < p_value <= 5.00e-02),\n",
        "        '**': (1.00e-03 < p_value <= 1.00e-02),\n",
        "        '***': (1.00e-04 < p_value <= 1.00e-03),\n",
        "        '****': (p_value <= 1.00e-04)\n",
        "    }\n",
        "    for key, interval_check in p_value_legend.items():\n",
        "        if interval_check:\n",
        "            return key\n",
        "    return None\n",
        "\n",
        "num_subplots = len(col_names_to_analyze)\n",
        "fig, axes = plt.subplots(1, num_subplots, figsize=(num_subplots * 5, 8), dpi=750)\n",
        "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
        "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
        "legend_handles = []\n",
        "\n",
        "for i, col_name in enumerate(col_names_to_analyze):\n",
        "    ax = axes[i]\n",
        "    sns.violinplot(data=one_big_merged_frame, x='Condition', y=col_name, hue='Condition', palette='pastel', ax=ax, showfliers=0, dodge=False)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=-90, fontsize = 12)\n",
        "    ax.set_title(col_name, pad=55)\n",
        "    ks_number = 3\n",
        "    sequence_of_data = list(range(1, ks_number+1))\n",
        "    \n",
        "    ks_annotations = [f'{sequence_of_data_pairs[k]}: {check_p_value_interval(ks_p_value_total[k+i*ks_number])}' for k in range(ks_number)]\n",
        "    ax.annotate('\\n'.join(ks_annotations),\n",
        "            xy=(0.4, 1), xycoords='axes fraction',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.5\", edgecolor='blue', facecolor='white'),\n",
        "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=0.3\", color='blue'))\n",
        "    ax.get_legend().remove()\n",
        "    ax.set_xlabel('')\n",
        "    if i == len(col_names_to_analyze) - 1:\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        legend_handles.extend(handles)\n",
        "legend = plt.legend(legend_handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "76166ee0aeba67162e2718088504f9357283de456324f11719e2ee59432b8cff"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
